{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, features\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ae import AutoEncoder\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/introduction')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using the device\",device)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def take_random_tile(img_raw,tile_size=8,sample_shape_range=None):\n",
    "    channels, width,height=list(img_raw.shape) #(3,width,height)\n",
    "\n",
    "    if sample_shape_range != None:\n",
    "        min_size, max_size = sample_shape_range\n",
    "\n",
    "        tile_size = randint(min_size,max_size)\n",
    "\n",
    "        if tile_size > width or tile_size > height: #correct for sample sizes bigger than the image\n",
    "            tile_size = min(height,width)\n",
    "    \n",
    "    top  = randint(0,width-tile_size)\n",
    "    left = randint(0,height-tile_size)\n",
    "\n",
    "    tile_tensor = img_raw[:,top:top+tile_size,left:left+tile_size] #tile\n",
    "    #print(list(tile_tensor.shape))\n",
    "\n",
    "    if sample_shape_range != None:\n",
    "        tile_tensor = F.interpolate(tile_tensor,size=(channels,tile_size,tile_size))\n",
    "    return tile_tensor\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "def get_tile_batch(image_list, batch_size=1000, tile_size=8, sample_shape_range=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    while True:\n",
    "        tile_list = [] #initialize batch tensor\n",
    "        for i in range(batch_size):\n",
    "            next_image,=sample(image_list,k=1)\n",
    "            image_tensor = convert_tensor(Image.open(next_image).convert('RGB'))\n",
    "            tile = take_random_tile(image_tensor,tile_size,sample_shape_range)\n",
    "            #print(tile.shape)\n",
    "            tile_list.append(tile)\n",
    "\n",
    "        batch = torch.stack(tile_list)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "\n",
    "wd_path = os.path.abspath(os.getcwd())\n",
    "root_path = os.path.join(wd_path, \"datasets\", 'ImageNet','train')\n",
    "\n",
    "\n",
    "image_path_list = []\n",
    "for d in listdir(root_path):\n",
    "    #d = n016...\n",
    "    class_path = join(root_path,d,\"images\")\n",
    "    for i in listdir(class_path):\n",
    "        #i = n016..._n.JPEG\n",
    "        image_path_list.append(join(class_path,i))\n",
    "\n",
    "batch_maker = get_tile_batch(image_path_list,batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "batch_maker = get_tile_batch(image_path_list,batch_size=8)\n",
    "sample_images = next(batch_maker)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(sample_images))\n",
    "# print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(epoch_size, train_loader,optimizer,model,criterion,max_epochs):\n",
    "    training_loss_history= []\n",
    "    loss = 0\n",
    "\n",
    "    for epoch_number,tiles in enumerate(train_loader):\n",
    "\n",
    "        # reshape mini-batch data to [N, 3*8*8] matrix\n",
    "        # load it to the active device\n",
    "        tiles = tiles.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(tiles)                 # compute reconstructions\n",
    "        train_loss = criterion(outputs, tiles.view(-1, 3*8*8)) # compute training reconstruction loss\n",
    "        train_loss.backward()                  # compute accumulated gradients\n",
    "        optimizer.step()                       # perform parameter update based on current gradients\n",
    "        loss += train_loss.item()              # add the mini-batch training loss to epoch loss\n",
    "        \n",
    "        if epoch_number%epoch_size == epoch_size-1:\n",
    "            # compute the epoch training loss\n",
    "            loss = loss / epoch_size\n",
    "\n",
    "            # display the epoch training loss\n",
    "            print(f\"epoch : {(epoch_number+1)/epoch_size}, loss = {loss:.6f}\")\n",
    "            training_loss_history.append(loss)\n",
    "            loss = 0\n",
    "            if(epoch_number >= max_epochs):\n",
    "                return training_loss_history\n",
    "    return training_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AutoEncoder()\n",
    "net.to(device)\n",
    "batch_maker = get_tile_batch(image_path_list,batch_size=1000)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_autoencoder(10, batch_maker, optimizer, net, criterion,max_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.path.join(wd_path, \"models\")\n",
    "\n",
    "net.save_model(os.path.join(models_path, \"ae_0.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEJUlEQVR4nO3dsXEjRxBA0dldgneKSZnJVRRSbBeHqAMIrBLAwVI1v/GeC6NnCXxMFQ30dp7nAnr2rz4A8Jw4IUqcECVOiBInRL29evGPP38f+1fu/timRq37521s1lpr3T6vY7Ou97lZt4/PsVnn59ystdZ6DH4e//7rx9Nhbk6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEvVzHcGxz7T6OuSW+9/vsT/tv2+CC4vtjbNR5zM36eZ1d8nxMvme/4OaEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1Mt1DPfz59Q51v12jM1a59wagbXWOve5Z9svl7FZx5pbWXC5zK5HOE/rGIBfECdEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROiXu5Kuf1znTrHug2upjhvc8+11lqPbXCnyDG3l+Xb2/exWY/z37FZa6113kfHPeXmhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTLdQzX6+Q6hsfYrDW4HmGttd73y9isy/tvY7P2wT/j2+V9btha6/H59fsY3JwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IermO4faY+73928fH2Kz9fW49wlpr3fe578DBzQ/rOAZXJAyvR9iP2ZUdT8/w1QcAnhMnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRL1cx7B9blPnWOvtGBt1Dn8lHcf3sVnn/eVb+r/a1mNs1r7NfT7WWut+zj3br7g5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiHq9WOOYa/dyfhubdWzvY7PWWusY3LuxD75nj21ul852zO5K2QP31tefAHhKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC1nef51WcAnnBzQpQ4IUqcECVOiBInRIkTov4DMh9o+yowFB8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEuElEQVR4nO3dP2vdZRjG8fuIjTYxTTWnrQGPTWvT2nYREyiKOunmIE6Ovgqx2IKTkx076NJ3IOiqk4NKxEUEpbFD8UCtiH/Q1NjYHt/AyZnK3Wv4fMbfGa6HwJcHsjyDyWRSQJ4H7vcBgOnECaHECaHECaHECaEenPXjm+fPtf0r98zZk11T9d2337RtVVXdney0bc0v7Gvb+v2PX9u2RqNR21ZV1d9/bbdtffju1mDadzcnhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhJr5HMMvP4+7zlGbX222bb38yrm2raqqza+/b9taHu5v25pfeKhta3f337atqqozZ0+17k3j5oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQM59j2Fh/pukYVRfeeatt6/1L77VtVVUdP7bStjU8tNS2tX3rz7at4aGDbVtVVbdv77TuTePmhFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFAz30o5sLjQdY66cPHttq2jq0+0bVVV/TS+1rY1PLzYtnXz5o22reXlR9u2qqqWlvr+jntxc0IocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUKomc8xbH75Rdc56vTTJ9u2TqwdbduqqhqPf2zbuvPfTtvW+sazbVtzc3NtW1VVW1tXW/emcXNCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCqJnPMTy18njXOerWP9ttW0ceOdi2VVU1Wj7StrU2OtG2dfXaD21bT672PqHx8UfX27Yun5/+3c0JocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJoWY+xzC829fuyvFTbVvHFg+3bVVVPX96o21r/775tq2FtfW2res3xm1bVVUvPXegdW8aNyeEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEGkwmkz1/XH14sPeP99irrz3WNVWffvZb21ZV1etvrLVtbbz4QtvWB1eutG3tNl8jd2a+InRvff7JZDDtu5sTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQs18jgG4f9ycEEqcEEqcEEqcEEqcEEqcEOp/b4phOfU4oYsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "batch_maker = get_tile_batch(image_path_list,batch_size=1)\n",
    "images = next(batch_maker)\n",
    "\n",
    "images = images.to(device)\n",
    "x_r = net.encode(images[0].view(3*8*8))\n",
    "x_rebuilt_r = net.decode(x_r).detach().cpu().view(3,8,8)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(x_rebuilt_r))\n",
    "print(\"Original\")\n",
    "imshow(images[0].detach().cpu().view(3,8,8))\n",
    "# print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (activation): ReLU()\n",
       "  (fc1): Linear(in_features=192, out_features=96, bias=True)\n",
       "  (fc2): Linear(in_features=96, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=96, bias=True)\n",
       "  (fc4): Linear(in_features=96, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_autoencoder = AutoEncoder.load_autoencoder(os.path.join(models_path, \"ae_0.pt\"))\n",
    "loaded_autoencoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEJUlEQVR4nO3dsXEjRxBA0dldgneKSZnJVRRSbBeHqAMIrBLAwVI1v/GeC6NnCXxMFQ30dp7nAnr2rz4A8Jw4IUqcECVOiBInRL29evGPP38f+1fu/timRq37521s1lpr3T6vY7Ou97lZt4/PsVnn59ystdZ6DH4e//7rx9Nhbk6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEvVzHcGxz7T6OuSW+9/vsT/tv2+CC4vtjbNR5zM36eZ1d8nxMvme/4OaEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1Mt1DPfz59Q51v12jM1a59wagbXWOve5Z9svl7FZx5pbWXC5zK5HOE/rGIBfECdEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROiXu5Kuf1znTrHug2upjhvc8+11lqPbXCnyDG3l+Xb2/exWY/z37FZa6113kfHPeXmhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTLdQzX6+Q6hsfYrDW4HmGttd73y9isy/tvY7P2wT/j2+V9btha6/H59fsY3JwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IermO4faY+73928fH2Kz9fW49wlpr3fe578DBzQ/rOAZXJAyvR9iP2ZUdT8/w1QcAnhMnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRL1cx7B9blPnWOvtGBt1Dn8lHcf3sVnn/eVb+r/a1mNs1r7NfT7WWut+zj3br7g5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiHq9WOOYa/dyfhubdWzvY7PWWusY3LuxD75nj21ul852zO5K2QP31tefAHhKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC1nef51WcAnnBzQpQ4IUqcECVOiBInRIkTov4DMh9o+yowFB8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEuElEQVR4nO3dP2vdZRjG8fuIjTYxTTWnrQGPTWvT2nYREyiKOunmIE6Ovgqx2IKTkx076NJ3IOiqk4NKxEUEpbFD8UCtiH/Q1NjYHt/AyZnK3Wv4fMbfGa6HwJcHsjyDyWRSQJ4H7vcBgOnECaHECaHECaHECaEenPXjm+fPtf0r98zZk11T9d2337RtVVXdney0bc0v7Gvb+v2PX9u2RqNR21ZV1d9/bbdtffju1mDadzcnhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhBInhJr5HMMvP4+7zlGbX222bb38yrm2raqqza+/b9taHu5v25pfeKhta3f337atqqozZ0+17k3j5oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQM59j2Fh/pukYVRfeeatt6/1L77VtVVUdP7bStjU8tNS2tX3rz7at4aGDbVtVVbdv77TuTePmhFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFDihFAz30o5sLjQdY66cPHttq2jq0+0bVVV/TS+1rY1PLzYtnXz5o22reXlR9u2qqqWlvr+jntxc0IocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUKomc8xbH75Rdc56vTTJ9u2TqwdbduqqhqPf2zbuvPfTtvW+sazbVtzc3NtW1VVW1tXW/emcXNCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCKHFCqJnPMTy18njXOerWP9ttW0ceOdi2VVU1Wj7StrU2OtG2dfXaD21bT672PqHx8UfX27Yun5/+3c0JocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJocQJoWY+xzC829fuyvFTbVvHFg+3bVVVPX96o21r/775tq2FtfW2res3xm1bVVUvPXegdW8aNyeEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEEieEGkwmkz1/XH14sPeP99irrz3WNVWffvZb21ZV1etvrLVtbbz4QtvWB1eutG3tNl8jd2a+InRvff7JZDDtu5sTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQokTQs18jgG4f9ycEEqcEEqcEEqcEEqcEEqcEOp/b4phOfU4oYsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "\n",
    "x_r = loaded_autoencoder.encode(images[0].view(3*8*8))\n",
    "x_rebuilt_r = loaded_autoencoder.decode(x_r).detach().cpu().view(3,8,8)\n",
    "\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(x_rebuilt_r))\n",
    "print(\"Original\")\n",
    "imshow(images[0].detach().cpu().view(3,8,8))\n",
    "# print labels"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80eecae6fe367bf31bafaafee0935a4935edb539e731c9082f4e2c31e5b4f550"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
